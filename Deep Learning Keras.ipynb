{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Keras\n",
    "# Xiaoqiao Zeng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# TensorFlow / Keras functions\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Ames Housing dataset. Create dummy variables for all of the categorical features. Print the first few rows of this dataset. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MS_SubClass</th>\n",
       "      <th>MS_Zoning</th>\n",
       "      <th>Lot_Frontage</th>\n",
       "      <th>Lot_Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot_Shape</th>\n",
       "      <th>Land_Contour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Lot_Config</th>\n",
       "      <th>...</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc_Feature</th>\n",
       "      <th>Misc_Val</th>\n",
       "      <th>Mo_Sold</th>\n",
       "      <th>Year_Sold</th>\n",
       "      <th>Sale_Type</th>\n",
       "      <th>Sale_Condition</th>\n",
       "      <th>Sale_Price</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>141</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "      <td>-93.619754</td>\n",
       "      <td>42.054035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_High_Density</td>\n",
       "      <td>80</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "      <td>-93.619756</td>\n",
       "      <td>42.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>81</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "      <td>-93.619387</td>\n",
       "      <td>42.052659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One_Story_1946_and_Newer_All_Styles</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>93</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Regular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>No_Fence</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "      <td>-93.617320</td>\n",
       "      <td>42.051245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two_Story_1946_and_Newer</td>\n",
       "      <td>Residential_Low_Density</td>\n",
       "      <td>74</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>No_Alley_Access</td>\n",
       "      <td>Slightly_Irregular</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>Minimum_Privacy</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "      <td>-93.638933</td>\n",
       "      <td>42.060899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MS_SubClass                 MS_Zoning  \\\n",
       "0  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "1  One_Story_1946_and_Newer_All_Styles  Residential_High_Density   \n",
       "2  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "3  One_Story_1946_and_Newer_All_Styles   Residential_Low_Density   \n",
       "4             Two_Story_1946_and_Newer   Residential_Low_Density   \n",
       "\n",
       "   Lot_Frontage  Lot_Area Street            Alley           Lot_Shape  \\\n",
       "0           141     31770   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "1            80     11622   Pave  No_Alley_Access             Regular   \n",
       "2            81     14267   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "3            93     11160   Pave  No_Alley_Access             Regular   \n",
       "4            74     13830   Pave  No_Alley_Access  Slightly_Irregular   \n",
       "\n",
       "  Land_Contour Utilities Lot_Config  ...            Fence Misc_Feature  \\\n",
       "0          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "1          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "2          Lvl    AllPub     Corner  ...         No_Fence         Gar2   \n",
       "3          Lvl    AllPub     Corner  ...         No_Fence         None   \n",
       "4          Lvl    AllPub     Inside  ...  Minimum_Privacy         None   \n",
       "\n",
       "  Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price  Longitude  \\\n",
       "0        0       5      2010       WD          Normal     215000 -93.619754   \n",
       "1        0       6      2010       WD          Normal     105000 -93.619756   \n",
       "2    12500       6      2010       WD          Normal     172000 -93.619387   \n",
       "3        0       4      2010       WD          Normal     244000 -93.617320   \n",
       "4        0       3      2010       WD          Normal     189900 -93.638933   \n",
       "\n",
       "    Latitude  \n",
       "0  42.054035  \n",
       "1  42.053014  \n",
       "2  42.052659  \n",
       "3  42.051245  \n",
       "4  42.060899  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames = pd.read_csv('ames.csv')\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lot_Frontage</th>\n",
       "      <th>Lot_Area</th>\n",
       "      <th>Year_Built</th>\n",
       "      <th>Year_Remod_Add</th>\n",
       "      <th>Mas_Vnr_Area</th>\n",
       "      <th>BsmtFin_SF_1</th>\n",
       "      <th>BsmtFin_SF_2</th>\n",
       "      <th>Bsmt_Unf_SF</th>\n",
       "      <th>Total_Bsmt_SF</th>\n",
       "      <th>First_Flr_SF</th>\n",
       "      <th>...</th>\n",
       "      <th>Sale_Type_ConLw</th>\n",
       "      <th>Sale_Type_New</th>\n",
       "      <th>Sale_Type_Oth</th>\n",
       "      <th>Sale_Type_VWD</th>\n",
       "      <th>Sale_Type_WD</th>\n",
       "      <th>Sale_Condition_AdjLand</th>\n",
       "      <th>Sale_Condition_Alloca</th>\n",
       "      <th>Sale_Condition_Family</th>\n",
       "      <th>Sale_Condition_Normal</th>\n",
       "      <th>Sale_Condition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>31770</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>441</td>\n",
       "      <td>1080</td>\n",
       "      <td>1656</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>11622</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>144</td>\n",
       "      <td>270</td>\n",
       "      <td>882</td>\n",
       "      <td>896</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81</td>\n",
       "      <td>14267</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>406</td>\n",
       "      <td>1329</td>\n",
       "      <td>1329</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>11160</td>\n",
       "      <td>1968</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1045</td>\n",
       "      <td>2110</td>\n",
       "      <td>2110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74</td>\n",
       "      <td>13830</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>928</td>\n",
       "      <td>928</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lot_Frontage  Lot_Area  Year_Built  Year_Remod_Add  Mas_Vnr_Area  \\\n",
       "0           141     31770        1960            1960           112   \n",
       "1            80     11622        1961            1961             0   \n",
       "2            81     14267        1958            1958           108   \n",
       "3            93     11160        1968            1968             0   \n",
       "4            74     13830        1997            1998             0   \n",
       "\n",
       "   BsmtFin_SF_1  BsmtFin_SF_2  Bsmt_Unf_SF  Total_Bsmt_SF  First_Flr_SF  ...  \\\n",
       "0             2             0          441           1080          1656  ...   \n",
       "1             6           144          270            882           896  ...   \n",
       "2             1             0          406           1329          1329  ...   \n",
       "3             1             0         1045           2110          2110  ...   \n",
       "4             3             0          137            928           928  ...   \n",
       "\n",
       "   Sale_Type_ConLw  Sale_Type_New  Sale_Type_Oth  Sale_Type_VWD  \\\n",
       "0                0              0              0              0   \n",
       "1                0              0              0              0   \n",
       "2                0              0              0              0   \n",
       "3                0              0              0              0   \n",
       "4                0              0              0              0   \n",
       "\n",
       "   Sale_Type_WD   Sale_Condition_AdjLand  Sale_Condition_Alloca  \\\n",
       "0              1                       0                      0   \n",
       "1              1                       0                      0   \n",
       "2              1                       0                      0   \n",
       "3              1                       0                      0   \n",
       "4              1                       0                      0   \n",
       "\n",
       "   Sale_Condition_Family  Sale_Condition_Normal  Sale_Condition_Partial  \n",
       "0                      0                      1                       0  \n",
       "1                      0                      1                       0  \n",
       "2                      0                      1                       0  \n",
       "3                      0                      1                       0  \n",
       "4                      0                      1                       0  \n",
       "\n",
       "[5 rows x 307 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.get_dummies(ames, drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use \"Sale_Price\" to create a column vector of responses (y). Create a feature matrix (X) using all of the other variables. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(\"Sale_Price\", axis=1).values\n",
    "y=df.Sale_Price.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2930, 306)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2930, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[215000],\n",
       "       [105000],\n",
       "       [172000],\n",
       "       ...,\n",
       "       [132000],\n",
       "       [170000],\n",
       "       [188000]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the data into training, validation, and testing sets using a 60/20/20 split. Print the dimensions of each of the feature matrices. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]  # total sample size, n\n",
    "n20 = int(.2*n) # 20% of n\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=n20, random_state=1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=n20, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shapes:  (1758, 306) (586, 306) (586, 306)\n",
      "y shapes:  (1758, 1) (586, 1) (586, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X shapes: ', X_train.shape, X_valid.shape, X_test.shape)\n",
    "print('y shapes: ', y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Standardize both the features and responses from the training set. Apply the appropriate transformation to the validation and test sets. (10 pts)\n",
    "\n",
    "Note: standardizing the dummy variables is optional. It's OK if you leave them as is, and it's also OK to standardize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = StandardScaler()\n",
    "X_train = scaler_x.fit_transform(X_train)\n",
    "X_valid = scaler_x.transform(X_valid)\n",
    "X_test = scaler_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()  \n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_valid = scaler_y.transform(y_valid)\n",
    "y_test = scaler_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Before we fit a neural network, we'll first try a simpler model using the Lasso. Fit a Lasso regression model to the training set using alpha=.05 as the tuning parameter. This should be a good choice assuming the data has been standardized. Calculate the mean squared error (MSE) on the training, validation, and test sets. (10 pts)\n",
    "Note: We did not use the validation set yet, so this is just another test set. We will use one when training the neural network, but we shouldn't train the Lasso with it for a fair comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of training set: 0.15662416279515554\n",
      "MSE of valid set: 0.14813466441967296\n",
      "MSE of test set: 0.11620386153587978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso=Lasso(alpha=.05)\n",
    "lasso.fit(X_train, y_train)\n",
    "print('MSE of training set:', mean_squared_error(y_train, lasso.predict(X_train)))\n",
    "print('MSE of valid set:', mean_squared_error(y_valid, lasso.predict(X_valid)))\n",
    "print('MSE of test set:', mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fit a neural network using the following architecture, and print the output from training. (15 pts) Use 2 hidden layers with 50 neurons each; Use ReLU activation functions for the hidden layers; Use a linear activation function for the output layer; Use MSE for the loss function; Use the Adam optimizer; Use 100 epochs; Use a batch size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 50)                15350     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 17,951\n",
      "Trainable params: 17,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(50, activation='relu', input_shape=(X_train.shape[1], )))\n",
    "model1.add(Dense(50, activation='relu', input_shape=(X_train.shape[1], )))\n",
    "model1.add(Dense(1, activation='linear'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',\n",
    "               loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1758 samples, validate on 586 samples\n",
      "Epoch 1/100\n",
      "1758/1758 [==============================] - 1s 325us/sample - loss: 0.6980 - mean_squared_error: 0.6980 - val_loss: 0.3967 - val_mean_squared_error: 0.3967\n",
      "Epoch 2/100\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.2575 - mean_squared_error: 0.2575 - val_loss: 0.2768 - val_mean_squared_error: 0.2768\n",
      "Epoch 3/100\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.1575 - mean_squared_error: 0.1575 - val_loss: 0.2409 - val_mean_squared_error: 0.2409\n",
      "Epoch 4/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.1097 - mean_squared_error: 0.1097 - val_loss: 0.2233 - val_mean_squared_error: 0.2233\n",
      "Epoch 5/100\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0822 - mean_squared_error: 0.0822 - val_loss: 0.2143 - val_mean_squared_error: 0.2143\n",
      "Epoch 6/100\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.2075 - val_mean_squared_error: 0.2075\n",
      "Epoch 7/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0540 - mean_squared_error: 0.0540 - val_loss: 0.2062 - val_mean_squared_error: 0.2062\n",
      "Epoch 8/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0440 - mean_squared_error: 0.0440 - val_loss: 0.1997 - val_mean_squared_error: 0.1997\n",
      "Epoch 9/100\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0372 - mean_squared_error: 0.0372 - val_loss: 0.2028 - val_mean_squared_error: 0.2028\n",
      "Epoch 10/100\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0318 - mean_squared_error: 0.0318 - val_loss: 0.1979 - val_mean_squared_error: 0.1979\n",
      "Epoch 11/100\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0273 - mean_squared_error: 0.0273 - val_loss: 0.1967 - val_mean_squared_error: 0.1967\n",
      "Epoch 12/100\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0242 - mean_squared_error: 0.0242 - val_loss: 0.1961 - val_mean_squared_error: 0.1961\n",
      "Epoch 13/100\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.1930 - val_mean_squared_error: 0.1930\n",
      "Epoch 14/100\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.1927 - val_mean_squared_error: 0.1927\n",
      "Epoch 15/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.1923 - val_mean_squared_error: 0.1923\n",
      "Epoch 16/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.1899 - val_mean_squared_error: 0.1899\n",
      "Epoch 17/100\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.1887 - val_mean_squared_error: 0.1887\n",
      "Epoch 18/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0152 - mean_squared_error: 0.0152 - val_loss: 0.1893 - val_mean_squared_error: 0.1893\n",
      "Epoch 19/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.1886 - val_mean_squared_error: 0.1886\n",
      "Epoch 20/100\n",
      "1758/1758 [==============================] - 0s 46us/sample - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.1883 - val_mean_squared_error: 0.1883\n",
      "Epoch 21/100\n",
      "1758/1758 [==============================] - 0s 55us/sample - loss: 0.0147 - mean_squared_error: 0.0147 - val_loss: 0.1868 - val_mean_squared_error: 0.1868\n",
      "Epoch 22/100\n",
      "1758/1758 [==============================] - 0s 57us/sample - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.1876 - val_mean_squared_error: 0.1876\n",
      "Epoch 23/100\n",
      "1758/1758 [==============================] - 0s 67us/sample - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.1859 - val_mean_squared_error: 0.1859\n",
      "Epoch 24/100\n",
      "1758/1758 [==============================] - 0s 52us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.1850 - val_mean_squared_error: 0.1850\n",
      "Epoch 25/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.1847 - val_mean_squared_error: 0.1847\n",
      "Epoch 26/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.1873 - val_mean_squared_error: 0.1873\n",
      "Epoch 27/100\n",
      "1758/1758 [==============================] - 0s 57us/sample - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.1855 - val_mean_squared_error: 0.1855\n",
      "Epoch 28/100\n",
      "1758/1758 [==============================] - 0s 61us/sample - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.1840 - val_mean_squared_error: 0.1840\n",
      "Epoch 29/100\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.1831 - val_mean_squared_error: 0.1831\n",
      "Epoch 30/100\n",
      "1758/1758 [==============================] - 0s 56us/sample - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.1838 - val_mean_squared_error: 0.1838\n",
      "Epoch 31/100\n",
      "1758/1758 [==============================] - 0s 56us/sample - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.1839 - val_mean_squared_error: 0.1839\n",
      "Epoch 32/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.1835 - val_mean_squared_error: 0.1835\n",
      "Epoch 33/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.1828 - val_mean_squared_error: 0.1828\n",
      "Epoch 34/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.1857 - val_mean_squared_error: 0.1857\n",
      "Epoch 35/100\n",
      "1758/1758 [==============================] - 0s 53us/sample - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.1829 - val_mean_squared_error: 0.1829\n",
      "Epoch 36/100\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.1876 - val_mean_squared_error: 0.1876\n",
      "Epoch 37/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.1797 - val_mean_squared_error: 0.1797\n",
      "Epoch 38/100\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.1850 - val_mean_squared_error: 0.1850\n",
      "Epoch 39/100\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.1800 - val_mean_squared_error: 0.1800\n",
      "Epoch 40/100\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.1833 - val_mean_squared_error: 0.1833\n",
      "Epoch 41/100\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.1806 - val_mean_squared_error: 0.1806\n",
      "Epoch 42/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.1809 - val_mean_squared_error: 0.1809\n",
      "Epoch 43/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.1808 - val_mean_squared_error: 0.1808\n",
      "Epoch 44/100\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.1805 - val_mean_squared_error: 0.1805\n",
      "Epoch 45/100\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.1813 - val_mean_squared_error: 0.1813\n",
      "Epoch 46/100\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.1805 - val_mean_squared_error: 0.1805\n",
      "Epoch 47/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.1803 - val_mean_squared_error: 0.1803\n",
      "Epoch 48/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.1821 - val_mean_squared_error: 0.1821\n",
      "Epoch 49/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.1809 - val_mean_squared_error: 0.1809\n",
      "Epoch 50/100\n",
      "1758/1758 [==============================] - 0s 63us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.1810 - val_mean_squared_error: 0.1810\n",
      "Epoch 51/100\n",
      "1758/1758 [==============================] - 0s 51us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.1812 - val_mean_squared_error: 0.1812\n",
      "Epoch 52/100\n",
      "1758/1758 [==============================] - 0s 56us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.1813 - val_mean_squared_error: 0.1813\n",
      "Epoch 53/100\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.1800 - val_mean_squared_error: 0.1800\n",
      "Epoch 54/100\n",
      "1758/1758 [==============================] - 0s 55us/sample - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.1826 - val_mean_squared_error: 0.1826\n",
      "Epoch 55/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.1814 - val_mean_squared_error: 0.1814\n",
      "Epoch 56/100\n",
      "1758/1758 [==============================] - 0s 58us/sample - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.1827 - val_mean_squared_error: 0.1827\n",
      "Epoch 57/100\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.1870 - val_mean_squared_error: 0.1870\n",
      "Epoch 58/100\n",
      "1758/1758 [==============================] - 0s 53us/sample - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.1806 - val_mean_squared_error: 0.1806\n",
      "Epoch 59/100\n",
      "1758/1758 [==============================] - 0s 59us/sample - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.1816 - val_mean_squared_error: 0.1816\n",
      "Epoch 60/100\n",
      "1758/1758 [==============================] - 0s 48us/sample - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.1836 - val_mean_squared_error: 0.1836\n",
      "Epoch 61/100\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.1828 - val_mean_squared_error: 0.1828\n",
      "Epoch 62/100\n",
      "1758/1758 [==============================] - 0s 45us/sample - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.1823 - val_mean_squared_error: 0.1823\n",
      "Epoch 63/100\n",
      "1758/1758 [==============================] - 0s 56us/sample - loss: 0.0115 - mean_squared_error: 0.0115 - val_loss: 0.1808 - val_mean_squared_error: 0.1808\n",
      "Epoch 64/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.1797 - val_mean_squared_error: 0.1797\n",
      "Epoch 65/100\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.1808 - val_mean_squared_error: 0.1808\n",
      "Epoch 66/100\n",
      "1758/1758 [==============================] - 0s 44us/sample - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.1794 - val_mean_squared_error: 0.1794\n",
      "Epoch 67/100\n",
      "1758/1758 [==============================] - 0s 59us/sample - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.1795 - val_mean_squared_error: 0.1795\n",
      "Epoch 68/100\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0118 - mean_squared_error: 0.0118 - val_loss: 0.1747 - val_mean_squared_error: 0.1747\n",
      "Epoch 69/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.1756 - val_mean_squared_error: 0.1756\n",
      "Epoch 70/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.1724 - val_mean_squared_error: 0.1724\n",
      "Epoch 71/100\n",
      "1758/1758 [==============================] - 0s 52us/sample - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.1764 - val_mean_squared_error: 0.1764\n",
      "Epoch 72/100\n",
      "1758/1758 [==============================] - 0s 56us/sample - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.1718 - val_mean_squared_error: 0.1718\n",
      "Epoch 73/100\n",
      "1758/1758 [==============================] - 0s 48us/sample - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.1749 - val_mean_squared_error: 0.1749\n",
      "Epoch 74/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.1721 - val_mean_squared_error: 0.1721\n",
      "Epoch 75/100\n",
      "1758/1758 [==============================] - 0s 66us/sample - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.1732 - val_mean_squared_error: 0.1732\n",
      "Epoch 76/100\n",
      "1758/1758 [==============================] - 0s 45us/sample - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.1751 - val_mean_squared_error: 0.1751\n",
      "Epoch 77/100\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.1743 - val_mean_squared_error: 0.1743\n",
      "Epoch 78/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.1734 - val_mean_squared_error: 0.1734\n",
      "Epoch 79/100\n",
      "1758/1758 [==============================] - 0s 54us/sample - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.1750 - val_mean_squared_error: 0.1750\n",
      "Epoch 80/100\n",
      "1758/1758 [==============================] - 0s 55us/sample - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.1745 - val_mean_squared_error: 0.1745\n",
      "Epoch 81/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.1734 - val_mean_squared_error: 0.1734\n",
      "Epoch 82/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.1738 - val_mean_squared_error: 0.1738\n",
      "Epoch 83/100\n",
      "1758/1758 [==============================] - 0s 58us/sample - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.1732 - val_mean_squared_error: 0.1732\n",
      "Epoch 84/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.1755 - val_mean_squared_error: 0.1755\n",
      "Epoch 85/100\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.1728 - val_mean_squared_error: 0.1728\n",
      "Epoch 86/100\n",
      "1758/1758 [==============================] - 0s 44us/sample - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.1754 - val_mean_squared_error: 0.1754\n",
      "Epoch 87/100\n",
      "1758/1758 [==============================] - 0s 57us/sample - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.1727 - val_mean_squared_error: 0.1727\n",
      "Epoch 88/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.1752 - val_mean_squared_error: 0.1752\n",
      "Epoch 89/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.1739 - val_mean_squared_error: 0.1739\n",
      "Epoch 90/100\n",
      "1758/1758 [==============================] - 0s 46us/sample - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.1744 - val_mean_squared_error: 0.1744\n",
      "Epoch 91/100\n",
      "1758/1758 [==============================] - 0s 64us/sample - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.1749 - val_mean_squared_error: 0.1749\n",
      "Epoch 92/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.1751 - val_mean_squared_error: 0.1751\n",
      "Epoch 93/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.1769 - val_mean_squared_error: 0.1769\n",
      "Epoch 94/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.1741 - val_mean_squared_error: 0.1741\n",
      "Epoch 95/100\n",
      "1758/1758 [==============================] - 0s 53us/sample - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.1749 - val_mean_squared_error: 0.1749\n",
      "Epoch 96/100\n",
      "1758/1758 [==============================] - 0s 50us/sample - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.1747 - val_mean_squared_error: 0.1747\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758/1758 [==============================] - 0s 51us/sample - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.1737 - val_mean_squared_error: 0.1737\n",
      "Epoch 98/100\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.1749 - val_mean_squared_error: 0.1749\n",
      "Epoch 99/100\n",
      "1758/1758 [==============================] - 0s 57us/sample - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.1728 - val_mean_squared_error: 0.1728\n",
      "Epoch 100/100\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.1742 - val_mean_squared_error: 0.1742\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(X_train, y_train, epochs=100, batch_size=100,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Print the MSE from the test set using the model in (6). How does this compare to the Lasso? (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of test set: 0.13776470854712644\n"
     ]
    }
   ],
   "source": [
    "print('MSE of test set:', mean_squared_error(y_test, model1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE ratio of model1 to Lasso model: 1.1855432919893925\n"
     ]
    }
   ],
   "source": [
    "print('MSE ratio of model1 to Lasso model:', mean_squared_error(y_test, model1.predict(X_test))/mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The MSE is a little bit larger than Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Optimize the neural network you fit in Q6 to get a better MSE than the Lasso model. You should use the validation set to ensure you're not overfitting. Print the output from training this model. Consider changing the following components to improve performance (it's not required to change all of these). (20 pts)\n",
    "Number of layers;\n",
    "Number of neurons per layer;\n",
    "Number of epochs and batch size;\n",
    "Activation functions of hidden layers;\n",
    "Adding regularization (Dropout layers, L1/L2 penalties, early stopping).\n",
    "Note: if for some reason the neural net from Q6 is already better than the Lasso model, then you should instead attempt to reduce the neural net's test MSE by 1%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc6c12ab2e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model2.add(Dense(100, activation='relu', input_shape=(X_train.shape[1], )))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(50, activation='relu', input_shape=(X_train.shape[1], )))\n",
    "model2.add(Dense(50, activation='relu',  input_shape=(X_train.shape[1], )))\n",
    "#model2.add(Dense(100, activation='relu', input_shape=(X_train.shape[1], )))\n",
    "model2.add(Dropout(0.5, input_shape=(X_train.shape[1], )))\n",
    "model2.add(Dense(1, activation='linear'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "               loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1758 samples, validate on 586 samples\n",
      "Epoch 1/200\n",
      "1758/1758 [==============================] - 1s 399us/sample - loss: 1.6670 - mean_squared_error: 1.6670 - val_loss: 0.6975 - val_mean_squared_error: 0.6975\n",
      "Epoch 2/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 1.0358 - mean_squared_error: 1.0358 - val_loss: 0.4688 - val_mean_squared_error: 0.4688\n",
      "Epoch 3/200\n",
      "1758/1758 [==============================] - 0s 24us/sample - loss: 0.7212 - mean_squared_error: 0.7212 - val_loss: 0.3471 - val_mean_squared_error: 0.3471\n",
      "Epoch 4/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.6353 - mean_squared_error: 0.6353 - val_loss: 0.3212 - val_mean_squared_error: 0.3212\n",
      "Epoch 5/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.5533 - mean_squared_error: 0.5533 - val_loss: 0.2951 - val_mean_squared_error: 0.2951\n",
      "Epoch 6/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.4236 - mean_squared_error: 0.4236 - val_loss: 0.2677 - val_mean_squared_error: 0.2677\n",
      "Epoch 7/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.4353 - mean_squared_error: 0.4353 - val_loss: 0.2500 - val_mean_squared_error: 0.2500\n",
      "Epoch 8/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.3595 - mean_squared_error: 0.3595 - val_loss: 0.2475 - val_mean_squared_error: 0.2475\n",
      "Epoch 9/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.3272 - mean_squared_error: 0.3272 - val_loss: 0.2322 - val_mean_squared_error: 0.2322\n",
      "Epoch 10/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.2806 - mean_squared_error: 0.2806 - val_loss: 0.2224 - val_mean_squared_error: 0.2224\n",
      "Epoch 11/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.3017 - mean_squared_error: 0.3017 - val_loss: 0.2136 - val_mean_squared_error: 0.2136\n",
      "Epoch 12/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.2662 - mean_squared_error: 0.2662 - val_loss: 0.2066 - val_mean_squared_error: 0.2066\n",
      "Epoch 13/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.2514 - mean_squared_error: 0.2514 - val_loss: 0.1980 - val_mean_squared_error: 0.1980\n",
      "Epoch 14/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.2322 - mean_squared_error: 0.2322 - val_loss: 0.1980 - val_mean_squared_error: 0.1980\n",
      "Epoch 15/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.2046 - mean_squared_error: 0.2046 - val_loss: 0.1977 - val_mean_squared_error: 0.1977\n",
      "Epoch 16/200\n",
      "1758/1758 [==============================] - 0s 24us/sample - loss: 0.2115 - mean_squared_error: 0.2115 - val_loss: 0.1879 - val_mean_squared_error: 0.1879\n",
      "Epoch 17/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.2089 - mean_squared_error: 0.2089 - val_loss: 0.1791 - val_mean_squared_error: 0.1791\n",
      "Epoch 18/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.2027 - mean_squared_error: 0.2027 - val_loss: 0.1751 - val_mean_squared_error: 0.1751\n",
      "Epoch 19/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.1902 - mean_squared_error: 0.1902 - val_loss: 0.1708 - val_mean_squared_error: 0.1708\n",
      "Epoch 20/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.1747 - mean_squared_error: 0.1747 - val_loss: 0.1721 - val_mean_squared_error: 0.1721\n",
      "Epoch 21/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.1619 - mean_squared_error: 0.1619 - val_loss: 0.1742 - val_mean_squared_error: 0.1742\n",
      "Epoch 22/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.1536 - mean_squared_error: 0.1536 - val_loss: 0.1736 - val_mean_squared_error: 0.1736\n",
      "Epoch 23/200\n",
      "1758/1758 [==============================] - 0s 24us/sample - loss: 0.1479 - mean_squared_error: 0.1479 - val_loss: 0.1706 - val_mean_squared_error: 0.1706\n",
      "Epoch 24/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.1425 - mean_squared_error: 0.1425 - val_loss: 0.1674 - val_mean_squared_error: 0.1674\n",
      "Epoch 25/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.1490 - mean_squared_error: 0.1490 - val_loss: 0.1661 - val_mean_squared_error: 0.1661\n",
      "Epoch 26/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1483 - mean_squared_error: 0.1483 - val_loss: 0.1708 - val_mean_squared_error: 0.1708\n",
      "Epoch 27/200\n",
      "1758/1758 [==============================] - 0s 23us/sample - loss: 0.1595 - mean_squared_error: 0.1595 - val_loss: 0.1663 - val_mean_squared_error: 0.1663\n",
      "Epoch 28/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.1565 - mean_squared_error: 0.1565 - val_loss: 0.1635 - val_mean_squared_error: 0.1635\n",
      "Epoch 29/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.1323 - mean_squared_error: 0.1323 - val_loss: 0.1671 - val_mean_squared_error: 0.1671\n",
      "Epoch 30/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.1266 - mean_squared_error: 0.1266 - val_loss: 0.1634 - val_mean_squared_error: 0.1634\n",
      "Epoch 31/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.1503 - mean_squared_error: 0.1503 - val_loss: 0.1610 - val_mean_squared_error: 0.1610\n",
      "Epoch 32/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.1338 - mean_squared_error: 0.1338 - val_loss: 0.1662 - val_mean_squared_error: 0.1662\n",
      "Epoch 33/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.1170 - mean_squared_error: 0.1170 - val_loss: 0.1612 - val_mean_squared_error: 0.1612\n",
      "Epoch 34/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.1275 - mean_squared_error: 0.1275 - val_loss: 0.1584 - val_mean_squared_error: 0.1584\n",
      "Epoch 35/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.1321 - mean_squared_error: 0.1321 - val_loss: 0.1585 - val_mean_squared_error: 0.1585\n",
      "Epoch 36/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.1137 - mean_squared_error: 0.1137 - val_loss: 0.1552 - val_mean_squared_error: 0.1552\n",
      "Epoch 37/200\n",
      "1758/1758 [==============================] - 0s 44us/sample - loss: 0.1202 - mean_squared_error: 0.1202 - val_loss: 0.1562 - val_mean_squared_error: 0.1562\n",
      "Epoch 38/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.1112 - mean_squared_error: 0.1112 - val_loss: 0.1549 - val_mean_squared_error: 0.1549\n",
      "Epoch 39/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1275 - mean_squared_error: 0.1275 - val_loss: 0.1526 - val_mean_squared_error: 0.1526\n",
      "Epoch 40/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.1249 - mean_squared_error: 0.1249 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 41/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1338 - mean_squared_error: 0.1338 - val_loss: 0.1562 - val_mean_squared_error: 0.1562\n",
      "Epoch 42/200\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.1191 - mean_squared_error: 0.1191 - val_loss: 0.1514 - val_mean_squared_error: 0.1514\n",
      "Epoch 43/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.1140 - mean_squared_error: 0.1140 - val_loss: 0.1534 - val_mean_squared_error: 0.1534\n",
      "Epoch 44/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1185 - mean_squared_error: 0.1185 - val_loss: 0.1542 - val_mean_squared_error: 0.1542\n",
      "Epoch 45/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.1009 - mean_squared_error: 0.1009 - val_loss: 0.1591 - val_mean_squared_error: 0.1591\n",
      "Epoch 46/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.1166 - mean_squared_error: 0.1166 - val_loss: 0.1545 - val_mean_squared_error: 0.1545\n",
      "Epoch 47/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.1151 - mean_squared_error: 0.1151 - val_loss: 0.1504 - val_mean_squared_error: 0.1504\n",
      "Epoch 48/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.1148 - mean_squared_error: 0.1148 - val_loss: 0.1508 - val_mean_squared_error: 0.1508\n",
      "Epoch 49/200\n",
      "1758/1758 [==============================] - 0s 51us/sample - loss: 0.1011 - mean_squared_error: 0.1011 - val_loss: 0.1567 - val_mean_squared_error: 0.1567\n",
      "Epoch 50/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1276 - mean_squared_error: 0.1276 - val_loss: 0.1499 - val_mean_squared_error: 0.1499\n",
      "Epoch 51/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0921 - mean_squared_error: 0.0921 - val_loss: 0.1525 - val_mean_squared_error: 0.1525\n",
      "Epoch 52/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.1054 - mean_squared_error: 0.1054 - val_loss: 0.1488 - val_mean_squared_error: 0.1488\n",
      "Epoch 53/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.1111 - mean_squared_error: 0.1111 - val_loss: 0.1513 - val_mean_squared_error: 0.1513\n",
      "Epoch 54/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.1043 - mean_squared_error: 0.1043 - val_loss: 0.1518 - val_mean_squared_error: 0.1518\n",
      "Epoch 55/200\n",
      "1758/1758 [==============================] - 0s 40us/sample - loss: 0.0992 - mean_squared_error: 0.0992 - val_loss: 0.1465 - val_mean_squared_error: 0.1465\n",
      "Epoch 56/200\n",
      "1758/1758 [==============================] - 0s 44us/sample - loss: 0.0991 - mean_squared_error: 0.0991 - val_loss: 0.1517 - val_mean_squared_error: 0.1517\n",
      "Epoch 57/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0918 - mean_squared_error: 0.0918 - val_loss: 0.1512 - val_mean_squared_error: 0.1512\n",
      "Epoch 58/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0981 - mean_squared_error: 0.0981 - val_loss: 0.1488 - val_mean_squared_error: 0.1488\n",
      "Epoch 59/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1070 - mean_squared_error: 0.1070 - val_loss: 0.1536 - val_mean_squared_error: 0.1536\n",
      "Epoch 60/200\n",
      "1758/1758 [==============================] - 0s 45us/sample - loss: 0.0973 - mean_squared_error: 0.0973 - val_loss: 0.1485 - val_mean_squared_error: 0.1485\n",
      "Epoch 61/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0929 - mean_squared_error: 0.0929 - val_loss: 0.1512 - val_mean_squared_error: 0.1512\n",
      "Epoch 62/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0952 - mean_squared_error: 0.0952 - val_loss: 0.1545 - val_mean_squared_error: 0.1545\n",
      "Epoch 63/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0982 - mean_squared_error: 0.0982 - val_loss: 0.1513 - val_mean_squared_error: 0.1513\n",
      "Epoch 64/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0925 - mean_squared_error: 0.0925 - val_loss: 0.1542 - val_mean_squared_error: 0.1542\n",
      "Epoch 65/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.1053 - mean_squared_error: 0.1053 - val_loss: 0.1462 - val_mean_squared_error: 0.1462\n",
      "Epoch 66/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0950 - mean_squared_error: 0.0950 - val_loss: 0.1499 - val_mean_squared_error: 0.1499\n",
      "Epoch 67/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0966 - mean_squared_error: 0.0966 - val_loss: 0.1463 - val_mean_squared_error: 0.1463\n",
      "Epoch 68/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0954 - mean_squared_error: 0.0954 - val_loss: 0.1472 - val_mean_squared_error: 0.1472\n",
      "Epoch 69/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0947 - mean_squared_error: 0.0947 - val_loss: 0.1441 - val_mean_squared_error: 0.1441\n",
      "Epoch 70/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0956 - mean_squared_error: 0.0956 - val_loss: 0.1394 - val_mean_squared_error: 0.1394\n",
      "Epoch 71/200\n",
      "1758/1758 [==============================] - 0s 44us/sample - loss: 0.0850 - mean_squared_error: 0.0850 - val_loss: 0.1407 - val_mean_squared_error: 0.1407\n",
      "Epoch 72/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0878 - mean_squared_error: 0.0878 - val_loss: 0.1381 - val_mean_squared_error: 0.1381\n",
      "Epoch 73/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.1027 - mean_squared_error: 0.1027 - val_loss: 0.1412 - val_mean_squared_error: 0.1412\n",
      "Epoch 74/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0935 - mean_squared_error: 0.0935 - val_loss: 0.1397 - val_mean_squared_error: 0.1397\n",
      "Epoch 75/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0860 - mean_squared_error: 0.0860 - val_loss: 0.1383 - val_mean_squared_error: 0.1383\n",
      "Epoch 76/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0922 - mean_squared_error: 0.0922 - val_loss: 0.1383 - val_mean_squared_error: 0.1383\n",
      "Epoch 77/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0789 - mean_squared_error: 0.0789 - val_loss: 0.1379 - val_mean_squared_error: 0.1379\n",
      "Epoch 78/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0858 - mean_squared_error: 0.0858 - val_loss: 0.1380 - val_mean_squared_error: 0.1380\n",
      "Epoch 79/200\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0952 - mean_squared_error: 0.0952 - val_loss: 0.1451 - val_mean_squared_error: 0.1451\n",
      "Epoch 80/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.1351 - val_mean_squared_error: 0.1351\n",
      "Epoch 81/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0894 - mean_squared_error: 0.0894 - val_loss: 0.1367 - val_mean_squared_error: 0.1367\n",
      "Epoch 82/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0908 - mean_squared_error: 0.0908 - val_loss: 0.1403 - val_mean_squared_error: 0.1403\n",
      "Epoch 83/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0896 - mean_squared_error: 0.0896 - val_loss: 0.1429 - val_mean_squared_error: 0.1429\n",
      "Epoch 84/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0867 - mean_squared_error: 0.0867 - val_loss: 0.1424 - val_mean_squared_error: 0.1424\n",
      "Epoch 85/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0948 - mean_squared_error: 0.0948 - val_loss: 0.1407 - val_mean_squared_error: 0.1407\n",
      "Epoch 86/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.1380 - val_mean_squared_error: 0.1380\n",
      "Epoch 87/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0827 - mean_squared_error: 0.0827 - val_loss: 0.1394 - val_mean_squared_error: 0.1394\n",
      "Epoch 88/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0895 - mean_squared_error: 0.0895 - val_loss: 0.1351 - val_mean_squared_error: 0.1351\n",
      "Epoch 89/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0919 - mean_squared_error: 0.0919 - val_loss: 0.1357 - val_mean_squared_error: 0.1357\n",
      "Epoch 90/200\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0856 - mean_squared_error: 0.0856 - val_loss: 0.1350 - val_mean_squared_error: 0.1350\n",
      "Epoch 91/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.0738 - mean_squared_error: 0.0738 - val_loss: 0.1365 - val_mean_squared_error: 0.1365\n",
      "Epoch 92/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0817 - mean_squared_error: 0.0817 - val_loss: 0.1359 - val_mean_squared_error: 0.1359\n",
      "Epoch 93/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0853 - mean_squared_error: 0.0853 - val_loss: 0.1409 - val_mean_squared_error: 0.1409\n",
      "Epoch 94/200\n",
      "1758/1758 [==============================] - 0s 40us/sample - loss: 0.0814 - mean_squared_error: 0.0814 - val_loss: 0.1341 - val_mean_squared_error: 0.1341\n",
      "Epoch 95/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0786 - mean_squared_error: 0.0786 - val_loss: 0.1337 - val_mean_squared_error: 0.1337\n",
      "Epoch 96/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0800 - mean_squared_error: 0.0800 - val_loss: 0.1338 - val_mean_squared_error: 0.1338\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.1326 - val_mean_squared_error: 0.1326\n",
      "Epoch 98/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0797 - mean_squared_error: 0.0797 - val_loss: 0.1401 - val_mean_squared_error: 0.1401\n",
      "Epoch 99/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0705 - mean_squared_error: 0.0705 - val_loss: 0.1329 - val_mean_squared_error: 0.1329\n",
      "Epoch 100/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0684 - mean_squared_error: 0.0684 - val_loss: 0.1359 - val_mean_squared_error: 0.1359\n",
      "Epoch 101/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.0742 - mean_squared_error: 0.0742 - val_loss: 0.1364 - val_mean_squared_error: 0.1364\n",
      "Epoch 102/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.1375 - val_mean_squared_error: 0.1375\n",
      "Epoch 103/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.0830 - mean_squared_error: 0.0830 - val_loss: 0.1343 - val_mean_squared_error: 0.1343\n",
      "Epoch 104/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0800 - mean_squared_error: 0.0800 - val_loss: 0.1296 - val_mean_squared_error: 0.1296\n",
      "Epoch 105/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0806 - mean_squared_error: 0.0806 - val_loss: 0.1372 - val_mean_squared_error: 0.1372\n",
      "Epoch 106/200\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0828 - mean_squared_error: 0.0828 - val_loss: 0.1306 - val_mean_squared_error: 0.1306\n",
      "Epoch 107/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0786 - mean_squared_error: 0.0786 - val_loss: 0.1396 - val_mean_squared_error: 0.1396\n",
      "Epoch 108/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0732 - mean_squared_error: 0.0732 - val_loss: 0.1388 - val_mean_squared_error: 0.1388\n",
      "Epoch 109/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0816 - mean_squared_error: 0.0816 - val_loss: 0.1344 - val_mean_squared_error: 0.1344\n",
      "Epoch 110/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0823 - mean_squared_error: 0.0823 - val_loss: 0.1411 - val_mean_squared_error: 0.1411\n",
      "Epoch 111/200\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0894 - mean_squared_error: 0.0894 - val_loss: 0.1353 - val_mean_squared_error: 0.1353\n",
      "Epoch 112/200\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0768 - mean_squared_error: 0.0768 - val_loss: 0.1383 - val_mean_squared_error: 0.1383\n",
      "Epoch 113/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0783 - mean_squared_error: 0.0783 - val_loss: 0.1357 - val_mean_squared_error: 0.1357\n",
      "Epoch 114/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.0672 - mean_squared_error: 0.0672 - val_loss: 0.1315 - val_mean_squared_error: 0.1315\n",
      "Epoch 115/200\n",
      "1758/1758 [==============================] - 0s 25us/sample - loss: 0.0725 - mean_squared_error: 0.0725 - val_loss: 0.1311 - val_mean_squared_error: 0.1311\n",
      "Epoch 116/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0715 - mean_squared_error: 0.0715 - val_loss: 0.1293 - val_mean_squared_error: 0.1293\n",
      "Epoch 117/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.0814 - mean_squared_error: 0.0814 - val_loss: 0.1338 - val_mean_squared_error: 0.1338\n",
      "Epoch 118/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.1329 - val_mean_squared_error: 0.1329\n",
      "Epoch 119/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0801 - mean_squared_error: 0.0801 - val_loss: 0.1350 - val_mean_squared_error: 0.1350\n",
      "Epoch 120/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.0839 - mean_squared_error: 0.0839 - val_loss: 0.1387 - val_mean_squared_error: 0.1387\n",
      "Epoch 121/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.0741 - mean_squared_error: 0.0741 - val_loss: 0.1401 - val_mean_squared_error: 0.1401\n",
      "Epoch 122/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0775 - mean_squared_error: 0.0775 - val_loss: 0.1353 - val_mean_squared_error: 0.1353\n",
      "Epoch 123/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0754 - mean_squared_error: 0.0754 - val_loss: 0.1347 - val_mean_squared_error: 0.1347\n",
      "Epoch 124/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0791 - mean_squared_error: 0.0791 - val_loss: 0.1355 - val_mean_squared_error: 0.1355\n",
      "Epoch 125/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0771 - mean_squared_error: 0.0771 - val_loss: 0.1388 - val_mean_squared_error: 0.1388\n",
      "Epoch 126/200\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0652 - mean_squared_error: 0.0652 - val_loss: 0.1411 - val_mean_squared_error: 0.1411\n",
      "Epoch 127/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0721 - mean_squared_error: 0.0721 - val_loss: 0.1447 - val_mean_squared_error: 0.1447\n",
      "Epoch 128/200\n",
      "1758/1758 [==============================] - ETA: 0s - loss: 0.0941 - mean_squared_error: 0.09 - 0s 36us/sample - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.1410 - val_mean_squared_error: 0.1410\n",
      "Epoch 129/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0734 - mean_squared_error: 0.0734 - val_loss: 0.1394 - val_mean_squared_error: 0.1394\n",
      "Epoch 130/200\n",
      "1758/1758 [==============================] - ETA: 0s - loss: 0.0934 - mean_squared_error: 0.09 - 0s 38us/sample - loss: 0.0707 - mean_squared_error: 0.0707 - val_loss: 0.1469 - val_mean_squared_error: 0.1469\n",
      "Epoch 131/200\n",
      "1758/1758 [==============================] - 0s 57us/sample - loss: 0.0763 - mean_squared_error: 0.0763 - val_loss: 0.1377 - val_mean_squared_error: 0.1377\n",
      "Epoch 132/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0771 - mean_squared_error: 0.0771 - val_loss: 0.1404 - val_mean_squared_error: 0.1404\n",
      "Epoch 133/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0748 - mean_squared_error: 0.0748 - val_loss: 0.1321 - val_mean_squared_error: 0.1321\n",
      "Epoch 134/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0733 - mean_squared_error: 0.0733 - val_loss: 0.1324 - val_mean_squared_error: 0.1324\n",
      "Epoch 135/200\n",
      "1758/1758 [==============================] - 0s 29us/sample - loss: 0.0712 - mean_squared_error: 0.0712 - val_loss: 0.1334 - val_mean_squared_error: 0.1334\n",
      "Epoch 136/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0739 - mean_squared_error: 0.0739 - val_loss: 0.1376 - val_mean_squared_error: 0.1376\n",
      "Epoch 137/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0670 - mean_squared_error: 0.0670 - val_loss: 0.1365 - val_mean_squared_error: 0.1365\n",
      "Epoch 138/200\n",
      "1758/1758 [==============================] - 0s 47us/sample - loss: 0.0717 - mean_squared_error: 0.0717 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 139/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0650 - mean_squared_error: 0.0650 - val_loss: 0.1315 - val_mean_squared_error: 0.1315\n",
      "Epoch 140/200\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0744 - mean_squared_error: 0.0744 - val_loss: 0.1339 - val_mean_squared_error: 0.1339\n",
      "Epoch 141/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0721 - mean_squared_error: 0.0721 - val_loss: 0.1332 - val_mean_squared_error: 0.1332\n",
      "Epoch 142/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0738 - mean_squared_error: 0.0738 - val_loss: 0.1352 - val_mean_squared_error: 0.1352\n",
      "Epoch 143/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0647 - mean_squared_error: 0.0647 - val_loss: 0.1344 - val_mean_squared_error: 0.1344\n",
      "Epoch 144/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0617 - mean_squared_error: 0.0617 - val_loss: 0.1375 - val_mean_squared_error: 0.1375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0676 - mean_squared_error: 0.0676 - val_loss: 0.1322 - val_mean_squared_error: 0.1322\n",
      "Epoch 146/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0718 - mean_squared_error: 0.0718 - val_loss: 0.1367 - val_mean_squared_error: 0.1367\n",
      "Epoch 147/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0742 - mean_squared_error: 0.0742 - val_loss: 0.1325 - val_mean_squared_error: 0.1325\n",
      "Epoch 148/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0702 - mean_squared_error: 0.0702 - val_loss: 0.1338 - val_mean_squared_error: 0.1338\n",
      "Epoch 149/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.1290 - val_mean_squared_error: 0.1290\n",
      "Epoch 150/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0735 - mean_squared_error: 0.0735 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 151/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0665 - mean_squared_error: 0.0665 - val_loss: 0.1365 - val_mean_squared_error: 0.1365\n",
      "Epoch 152/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0662 - mean_squared_error: 0.0662 - val_loss: 0.1272 - val_mean_squared_error: 0.1272\n",
      "Epoch 153/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0691 - mean_squared_error: 0.0691 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 154/200\n",
      "1758/1758 [==============================] - 0s 40us/sample - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.1259 - val_mean_squared_error: 0.1259\n",
      "Epoch 155/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0751 - mean_squared_error: 0.0751 - val_loss: 0.1251 - val_mean_squared_error: 0.1251\n",
      "Epoch 156/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0674 - mean_squared_error: 0.0674 - val_loss: 0.1269 - val_mean_squared_error: 0.1269\n",
      "Epoch 157/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0742 - mean_squared_error: 0.0742 - val_loss: 0.1229 - val_mean_squared_error: 0.1229\n",
      "Epoch 158/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0771 - mean_squared_error: 0.0771 - val_loss: 0.1266 - val_mean_squared_error: 0.1266\n",
      "Epoch 159/200\n",
      "1758/1758 [==============================] - 0s 49us/sample - loss: 0.0635 - mean_squared_error: 0.0635 - val_loss: 0.1271 - val_mean_squared_error: 0.1271\n",
      "Epoch 160/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0629 - mean_squared_error: 0.0629 - val_loss: 0.1248 - val_mean_squared_error: 0.1248\n",
      "Epoch 161/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0739 - mean_squared_error: 0.0739 - val_loss: 0.1251 - val_mean_squared_error: 0.1251\n",
      "Epoch 162/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0640 - mean_squared_error: 0.0640 - val_loss: 0.1270 - val_mean_squared_error: 0.1270\n",
      "Epoch 163/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0646 - mean_squared_error: 0.0646 - val_loss: 0.1231 - val_mean_squared_error: 0.1231\n",
      "Epoch 164/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0639 - mean_squared_error: 0.0639 - val_loss: 0.1212 - val_mean_squared_error: 0.1212\n",
      "Epoch 165/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0636 - mean_squared_error: 0.0636 - val_loss: 0.1232 - val_mean_squared_error: 0.1232\n",
      "Epoch 166/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0670 - mean_squared_error: 0.0670 - val_loss: 0.1226 - val_mean_squared_error: 0.1226\n",
      "Epoch 167/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0713 - mean_squared_error: 0.0713 - val_loss: 0.1216 - val_mean_squared_error: 0.1216\n",
      "Epoch 168/200\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0751 - mean_squared_error: 0.0751 - val_loss: 0.1307 - val_mean_squared_error: 0.1307\n",
      "Epoch 169/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0729 - mean_squared_error: 0.0729 - val_loss: 0.1265 - val_mean_squared_error: 0.1265\n",
      "Epoch 170/200\n",
      "1758/1758 [==============================] - 0s 32us/sample - loss: 0.0703 - mean_squared_error: 0.0703 - val_loss: 0.1281 - val_mean_squared_error: 0.1281\n",
      "Epoch 171/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.1324 - val_mean_squared_error: 0.1324\n",
      "Epoch 172/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0671 - mean_squared_error: 0.0671 - val_loss: 0.1294 - val_mean_squared_error: 0.1294\n",
      "Epoch 173/200\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0732 - mean_squared_error: 0.0732 - val_loss: 0.1327 - val_mean_squared_error: 0.1327\n",
      "Epoch 174/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0653 - mean_squared_error: 0.0653 - val_loss: 0.1273 - val_mean_squared_error: 0.1273\n",
      "Epoch 175/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0634 - mean_squared_error: 0.0634 - val_loss: 0.1281 - val_mean_squared_error: 0.1281\n",
      "Epoch 176/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0712 - mean_squared_error: 0.0712 - val_loss: 0.1272 - val_mean_squared_error: 0.1272\n",
      "Epoch 177/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0785 - mean_squared_error: 0.0785 - val_loss: 0.1223 - val_mean_squared_error: 0.1223\n",
      "Epoch 178/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0719 - mean_squared_error: 0.0719 - val_loss: 0.1258 - val_mean_squared_error: 0.1258\n",
      "Epoch 179/200\n",
      "1758/1758 [==============================] - 0s 30us/sample - loss: 0.0728 - mean_squared_error: 0.0728 - val_loss: 0.1248 - val_mean_squared_error: 0.1248\n",
      "Epoch 180/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0676 - mean_squared_error: 0.0676 - val_loss: 0.1243 - val_mean_squared_error: 0.1243\n",
      "Epoch 181/200\n",
      "1758/1758 [==============================] - 0s 43us/sample - loss: 0.0733 - mean_squared_error: 0.0733 - val_loss: 0.1234 - val_mean_squared_error: 0.1234\n",
      "Epoch 182/200\n",
      "1758/1758 [==============================] - 0s 41us/sample - loss: 0.0636 - mean_squared_error: 0.0636 - val_loss: 0.1201 - val_mean_squared_error: 0.1201\n",
      "Epoch 183/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0689 - mean_squared_error: 0.0689 - val_loss: 0.1225 - val_mean_squared_error: 0.1225\n",
      "Epoch 184/200\n",
      "1758/1758 [==============================] - 0s 40us/sample - loss: 0.0602 - mean_squared_error: 0.0602 - val_loss: 0.1234 - val_mean_squared_error: 0.1234\n",
      "Epoch 185/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0654 - mean_squared_error: 0.0654 - val_loss: 0.1234 - val_mean_squared_error: 0.1234\n",
      "Epoch 186/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0723 - mean_squared_error: 0.0723 - val_loss: 0.1236 - val_mean_squared_error: 0.1236\n",
      "Epoch 187/200\n",
      "1758/1758 [==============================] - 0s 35us/sample - loss: 0.0663 - mean_squared_error: 0.0663 - val_loss: 0.1221 - val_mean_squared_error: 0.1221\n",
      "Epoch 188/200\n",
      "1758/1758 [==============================] - 0s 33us/sample - loss: 0.0644 - mean_squared_error: 0.0644 - val_loss: 0.1269 - val_mean_squared_error: 0.1268\n",
      "Epoch 189/200\n",
      "1758/1758 [==============================] - 0s 37us/sample - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.1245 - val_mean_squared_error: 0.1245\n",
      "Epoch 190/200\n",
      "1758/1758 [==============================] - 0s 39us/sample - loss: 0.0726 - mean_squared_error: 0.0726 - val_loss: 0.1304 - val_mean_squared_error: 0.1304\n",
      "Epoch 191/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0694 - mean_squared_error: 0.0694 - val_loss: 0.1286 - val_mean_squared_error: 0.1286\n",
      "Epoch 192/200\n",
      "1758/1758 [==============================] - 0s 34us/sample - loss: 0.0775 - mean_squared_error: 0.0775 - val_loss: 0.1267 - val_mean_squared_error: 0.1267\n",
      "Epoch 193/200\n",
      "1758/1758 [==============================] - 0s 36us/sample - loss: 0.0644 - mean_squared_error: 0.0644 - val_loss: 0.1245 - val_mean_squared_error: 0.1245\n",
      "Epoch 194/200\n",
      "1758/1758 [==============================] - 0s 31us/sample - loss: 0.0603 - mean_squared_error: 0.0603 - val_loss: 0.1236 - val_mean_squared_error: 0.1236\n",
      "Epoch 195/200\n",
      "1758/1758 [==============================] - 0s 27us/sample - loss: 0.0698 - mean_squared_error: 0.0698 - val_loss: 0.1267 - val_mean_squared_error: 0.1267\n",
      "Epoch 196/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.0654 - mean_squared_error: 0.0654 - val_loss: 0.1275 - val_mean_squared_error: 0.1275\n",
      "Epoch 197/200\n",
      "1758/1758 [==============================] - 0s 26us/sample - loss: 0.0700 - mean_squared_error: 0.0700 - val_loss: 0.1276 - val_mean_squared_error: 0.1276\n",
      "Epoch 198/200\n",
      "1758/1758 [==============================] - 0s 28us/sample - loss: 0.0637 - mean_squared_error: 0.0637 - val_loss: 0.1286 - val_mean_squared_error: 0.1286\n",
      "Epoch 199/200\n",
      "1758/1758 [==============================] - 0s 38us/sample - loss: 0.0625 - mean_squared_error: 0.0625 - val_loss: 0.1277 - val_mean_squared_error: 0.1277\n",
      "Epoch 200/200\n",
      "1758/1758 [==============================] - 0s 42us/sample - loss: 0.0611 - mean_squared_error: 0.0611 - val_loss: 0.1262 - val_mean_squared_error: 0.1262\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, y_train, epochs=200, batch_size=200,\n",
    "                     validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the MSE from the test set using the model in Q8. (5 pts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of test set: 0.10194027388038597\n"
     ]
    }
   ],
   "source": [
    "print('MSE of test set:', mean_squared_error(y_test, model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE ratio of optimized model to Lasso model: 0.8772537550218182\n"
     ]
    }
   ],
   "source": [
    "print('MSE ratio of optimized model to Lasso model:', mean_squared_error(y_test, model2.predict(X_test))/mean_squared_error(y_test, lasso.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Plot the training and validation MSE for each epoch. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.history.history['val_mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a37885c90>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model2.history.history['mean_squared_error'], label='Training MSE')\n",
    "plt.plot(model2.history.history['val_mean_squared_error'], label='Validation MSE')\n",
    "plt.legend()\n",
    "#validation may good for new tests set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. In a few sentences, describe how you optimized the model in Q8. Did you find that changing certain components had bigger effects on the error rate? (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I increased a hidden layer with 100 neurons, and also increased Number of epochs and batch size for larger neuron numbers, and received a larger MSE. Then I changed the added regularization L1/L2 penalties to layers, and I still got a larger MSE. Even with early stopping, the MSE increased still. So I felt the original number of neurons could be already high. Thus, I removed the addition hidden layer and add dropout layers, while keeping larger epochs and batch. Finally the MSE reduced lower than that of Lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. How do you know that your model is not overfitting? That is, how do you know you didn't get lucky on the test set? (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a38245dd0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model2.history.history['loss'], label='Training loss')\n",
    "plt.plot(model2.history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at both the validation output and the Train/validation MSE/loss plot. If an overfitting happens, the values of MSE and loss would show oscillating behaviors or went up from a lowest point. If it is not overfitted, the values would drop at the initial validation stage then keep stable at a specific level, like the orange curves shown in my plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
